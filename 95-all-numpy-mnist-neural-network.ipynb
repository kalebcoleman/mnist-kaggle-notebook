{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2314159,"sourceType":"datasetVersion","datasetId":1396483}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom scipy import ndimage\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:14:37.548593Z","iopub.execute_input":"2025-05-14T18:14:37.548983Z","iopub.status.idle":"2025-05-14T18:14:37.574026Z","shell.execute_reply.started":"2025-05-14T18:14:37.548956Z","shell.execute_reply":"2025-05-14T18:14:37.573042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chinese MNIST: Two‑Layer MLP with Mini‑Batch Adam + Augmentation + Early‑Stopping\n## All using numpy\nIn this notebook we will:\n*  Load and split the data into train / validation / test.\n*  Define helper functions (`iterate_minibatches`, `augment_batch_shifts`, `show_random_prediction`, `predict_random_samples`).\n*  Implement a mini‑batch Adam optimizer with on‑the‑fly random shifts and early stopping.\n*  Train the model and track train/test accuracy.\n*  Evaluate on the held‑out test set.\n* Visualize a few random predictions.","metadata":{}},{"cell_type":"markdown","source":"# Load & preprocess data","metadata":{}},{"cell_type":"code","source":"# 1.1 Read CSV into NumPy array\ndf = pd.read_csv('/kaggle/input/chinese-mnist-digit-recognizer/chineseMNIST.csv')\nraw = df.values              # shape should be (m, 4098)\nm, n = raw.shape\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:14:37.584001Z","iopub.execute_input":"2025-05-14T18:14:37.584316Z","iopub.status.idle":"2025-05-14T18:14:48.661903Z","shell.execute_reply.started":"2025-05-14T18:14:37.584294Z","shell.execute_reply":"2025-05-14T18:14:48.660872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1.2 Build label mappings\ndigit_labels = raw[:, 4096].astype(int)\nchar_labels  = raw[:, 4097].astype(str)\nunique_chars = sorted(set(char_labels))\nchar_to_idx  = { ch:i for i,ch in enumerate(unique_chars) }\nidx_to_char  = { i:ch for ch,i in char_to_idx.items() }\nN_CHARS      = len(unique_chars)\n\nprint(f\"Found {N_CHARS} distinct Chinese symbols: {unique_chars}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:14:48.663352Z","iopub.execute_input":"2025-05-14T18:14:48.664022Z","iopub.status.idle":"2025-05-14T18:14:48.678420Z","shell.execute_reply.started":"2025-05-14T18:14:48.663987Z","shell.execute_reply":"2025-05-14T18:14:48.677498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1.3 Three‑way split\ndef load_and_split_three(raw_data, test_size=1000, val_size=1000, seed=42):\n    \"\"\"\n    Splits raw_data into train, validation, and test sets.\n    raw_data: NumPy array of shape (m, 4098)\n    test_size: number of samples for final test set\n    val_size:  number of samples for validation set\n    seed:      random seed for reproducibility\n    Returns:\n      (X_train, Yd_train, Yc_train),\n      (X_val,   Yd_val,   Yc_val),\n      (X_test,  Yd_test,  Yc_test)\n    \"\"\"\n    # Extract pixels and labels\n    pixels = raw_data[:, :4096].astype(float)\n    X = pixels.T  # shape (4096, m)\n\n    Y_digit = raw_data[:, 4096].astype(int)\n    Y_char  = np.array([char_to_idx[ch] for ch in raw_data[:, 4097]], dtype=int)\n\n    # Shuffle indices\n    m = X.shape[1]\n    rng = np.random.default_rng(seed)\n    perm = rng.permutation(m)\n\n    # Compute split indices\n    test_idx = perm[:test_size]\n    val_idx  = perm[test_size:test_size + val_size]\n    train_idx= perm[test_size + val_size:]\n\n    # Create splits\n    X_test  = X[:, test_idx]\n    Yd_test = Y_digit[test_idx]\n    Yc_test = Y_char[test_idx]\n\n    X_val   = X[:, val_idx]\n    Yd_val  = Y_digit[val_idx]\n    Yc_val  = Y_char[val_idx]\n\n    X_train  = X[:, train_idx]\n    Yd_train = Y_digit[train_idx]\n    Yc_train = Y_char[train_idx]\n\n    return (X_train, Yd_train, Yc_train), (X_val, Yd_val, Yc_val), (X_test, Yd_test, Yc_test)\n\n(train_set, val_set, test_set) = load_and_split_three(raw, test_size=1000, val_size=1000)\nX_train, Yd_train, Yc_train = train_set\nX_val,   Yd_val,   Yc_val   = val_set\nX_test,  Yd_test,  Yc_test  = test_set\n\nprint(\"X_train:\", X_train.shape, \"Yc_train:\", Yc_train.shape)\nprint(\"X_val:  \", X_val.shape,   \"Yc_val:  \", Yc_val.shape)\nprint(\"X_test: \", X_test.shape,  \"Yc_test: \", Yc_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:14.459571Z","iopub.execute_input":"2025-05-14T18:15:14.459918Z","iopub.status.idle":"2025-05-14T18:15:18.701061Z","shell.execute_reply.started":"2025-05-14T18:15:14.459895Z","shell.execute_reply":"2025-05-14T18:15:18.700061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions\n\nBelow are our mini‑batch iterator, augmentation function, and a quick way to visualize a random test prediction.\n","metadata":{}},{"cell_type":"code","source":"# helper functions\n\n# function to iterate through mini-batches to help with over-fitting\ndef iterate_minibatches(X, Y, batch_size=64):\n    \"\"\"\n    Generator yielding mini-batches of (X, Y).\n    - X: numpy array of shape (n_x, m)\n    - Y: numpy array of shape (m,)\n    - batch_size: number of samples per batch\n    \"\"\"\n    m = X.shape[1]\n    perm = np.random.permutation(m)\n    for i in range(0, m, batch_size):\n        idx = perm[i:i + batch_size]\n        yield X[:, idx], Y[idx]\n\n# function to shift pictures to help with over-fitting \ndef augment_batch_shifts(Xb, max_shift=3):\n    \"\"\"\n    Xb: (4096, batch_size) flattened 64×64 images\n    returns: same shape, each image randomly rolled by up to ±max_shift pixels\n    \"\"\"\n    n_x, m = Xb.shape\n    H = W = int(np.sqrt(n_x))\n    imgs = Xb.reshape(H, W, m)\n    for i in range(m):\n        dx = np.random.randint(-max_shift, max_shift+1)\n        dy = np.random.randint(-max_shift, max_shift+1)\n        imgs[:, :, i] = np.roll(np.roll(imgs[:, :, i], dx, axis=0), dy, axis=1)\n    return imgs.reshape(n_x, m)\n\n# function to show what the current model parameters would predict\ndef show_random_prediction(W1, b1, W2, b2, X, Y, idx_to_char, forward_prop):\n    \"\"\"\n    Pick a random sample from (X, Y), run forward_prop, and display:\n      - the image\n      - the model's predicted character vs. the true character\n    \"\"\"\n    # Choose a random sample index\n    idx = np.random.randint(0, X.shape[1])\n    image_vec = X[:, idx]\n    image = image_vec.reshape(64, 64)  # reshape back to 64×64\n\n    # Forward pass for this single example\n    _, _, _, A2 = forward_prop(\n        W1, b1, W2, b2,\n        image_vec.reshape(-1, 1)  # shape (4096, 1)\n    )\n    probs = A2.flatten()\n\n    # Determine predicted and true labels\n    pred_idx = np.argmax(probs)\n    pred_char = idx_to_char[pred_idx]\n    true_char = idx_to_char[Y[idx]]\n\n    # Print out details\n    print(f\"Sample index: {idx}\")\n    print(\"Class probabilities:\", np.round(probs, 3))\n    print(f\"Predicted → char: {pred_char}\")\n    print(f\"True      → char: {true_char}\")\n\n    # Plot the image\n    plt.figure(figsize=(4, 4))\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\n# function I use to get prediction samples with the validation set\ndef predict_random_samples(\n    W1, b1, W2, b2,\n    X_val, Yc_val, Yd_val=None,\n    num_samples=8\n):\n    \"\"\"\n    Show `num_samples` random images from (X_val, Yc_val),\n    predict with the network, and display true vs predicted.\n    \"\"\"\n    m_val, n_val = X_val.shape[1], X_val.shape[0]\n    side = int(np.sqrt(n_val))                   #  sqrt(4096) = 64\n    idxs = np.random.choice(m_val, size=num_samples, replace=False)\n\n    # forward prop on entire val set once\n    _, _, _, A2_val = forward_prop(W1, b1, W2, b2, X_val)\n    preds_c = np.argmax(A2_val, axis=0)\n\n    plt.figure(figsize=(num_samples*1.5, 2))\n    for i, idx in enumerate(idxs):\n        img = X_val[:, idx].reshape(side, side)\n        true_c = Yc_val[idx]\n        pred_c = preds_c[idx]\n\n        ax = plt.subplot(1, num_samples, i+1)\n        ax.imshow(img, cmap='gray')\n        ax.axis('off')\n        ax.set_title(f\"T:{true_c}\\nP:{pred_c}\", fontsize=8)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:39:42.024029Z","iopub.execute_input":"2025-05-14T18:39:42.024747Z","iopub.status.idle":"2025-05-14T18:39:42.038478Z","shell.execute_reply.started":"2025-05-14T18:39:42.024723Z","shell.execute_reply":"2025-05-14T18:39:42.037308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture\n\n* Input layer: 4096 neurons (64×64 pixel values)\n\n* Hidden layer: 128 neurons, ReLU activation\n\n* Output layer: 15 neurons, Softmax activation (amount of characters)","metadata":{}},{"cell_type":"code","source":"# Variables for my nerual netowrk\nINPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE = 4096, 128, N_CHARS\n \ndef init_params():\n    W1 = np.random.randn(HIDDEN_SIZE, INPUT_SIZE) * np.sqrt(2.0/INPUT_SIZE)\n    b1 = np.zeros((HIDDEN_SIZE, 1))\n    W2 = np.random.randn(OUTPUT_SIZE, HIDDEN_SIZE) * np.sqrt(2.0/HIDDEN_SIZE)\n    b2 = np.zeros((OUTPUT_SIZE, 1))\n    return W1, b1, W2, b2\n\ndef ReLu(Z, derv=False):\n    if derv: return np.where(Z>0, 1, 0)\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    Zc   = Z - np.max(Z, axis=0, keepdims=True)\n    expZ = np.exp(Zc)\n    return expZ / np.sum(expZ, axis=0, keepdims=True)\n\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1 \n    A1 = ReLu(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef one_hot(Y, n_classes):\n    m = Y.size\n    Y_oh = np.zeros((n_classes, m))\n    Y_oh[Y, np.arange(m)] = 1\n    return Y_oh\n\ndef deriv_ReLu(Z):\n    return Z > 0\n    \ndef back_prop(W2, Z1, A1, Z2, A2, X, Y):\n    m    = Y.size\n    Y_oh = one_hot(Y, A2.shape[0])\n\n    dZ2  = A2 - Y_oh\n    dW2  = (1/m) * dZ2.dot(A1.T)\n    db2  = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dZ1  = W2.T.dot(dZ2) * (Z1 > 0)\n    dW1  = (1/m) * dZ1.dot(X.T)\n    db1  = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 -= alpha * dW1\n    b1 -= alpha * db1\n    W2 -= alpha * dW2\n    b2 -= alpha * db2\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:28.223674Z","iopub.execute_input":"2025-05-14T18:15:28.224497Z","iopub.status.idle":"2025-05-14T18:15:28.235556Z","shell.execute_reply.started":"2025-05-14T18:15:28.224460Z","shell.execute_reply":"2025-05-14T18:15:28.234523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Implementation Details\n\n### Key components:\n\n* init_params(): Initialization for weights, zeros for biases\n\n* forward_prop(): Computes layer outputs and activations\n\n* ReLu(): Applies the rectified linear unit activation function\n\n* softmax(): Converts logits into probability distributions over classes\n\n* back_prop(): Derives gradients for weights and biases\n\n* update_params(): Applies gradient descent updates","metadata":{}},{"cell_type":"code","source":"# Normal gradient descent\ndef gradient_descent(X, Y, iterations, alpha):\n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = back_prop(W2, Z1, A1, Z2, A2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 50 == 0:\n            preds = np.argmax(A2, axis=0)\n            acc   = np.mean(preds == Y)\n            print(f\"Iteration {i:4d} — accuracy with train data: {acc*100:5.2f}%\")\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:14:52.934842Z","iopub.execute_input":"2025-05-14T18:14:52.935343Z","iopub.status.idle":"2025-05-14T18:14:52.967337Z","shell.execute_reply.started":"2025-05-14T18:14:52.935309Z","shell.execute_reply":"2025-05-14T18:14:52.966382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train with gradient descent\nW1, b1, W2, b2 = gradient_descent(X_train, Yc_train,\n                                  iterations=500,\n                                  alpha=0.05)\n\n# Final evaluation on test set\n_, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\npreds = np.argmax(A2_test, axis=0)\ntest_acc = np.mean(preds == Yc_test)\nprint(f\"Test accuracy: {test_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:14:52.968612Z","iopub.execute_input":"2025-05-14T18:14:52.968986Z","iopub.status.idle":"2025-05-14T18:15:07.228271Z","shell.execute_reply.started":"2025-05-14T18:14:52.968956Z","shell.execute_reply":"2025-05-14T18:15:07.225861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is a simple 2 layer neural network and its accuracy ranges from about 30% to 45% over 500 iterations on the Chinese MNIST data set, this model on the english data set would be around 80% I want to improve upon this model and get it to atleast 85% accuracy","metadata":{}},{"cell_type":"code","source":"# Run the see the current model in action\n# The model will have poor performance at this stage\nshow_random_prediction(\n    W1, b1, W2, b2,\n    X_test, Yc_test,\n    idx_to_char,\n    forward_prop\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.229014Z","iopub.status.idle":"2025-05-14T18:15:07.229330Z","shell.execute_reply.started":"2025-05-14T18:15:07.229197Z","shell.execute_reply":"2025-05-14T18:15:07.229210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adam optimization for 2‐layer network\ndef gradient_descent_adam(\n    X_train, Y_train,\n    X_test,   Y_test,\n    iterations=500,\n    alpha=0.001,\n    beta1=0.9, beta2=0.999,\n    epsilon=1e-8,\n    print_every=50\n):\n    \"\"\"\n    Adam optimizer with train & validation logging.\n      - X_train, Y_train: training data\n      - X_test,   Y_test:   validation data for monitoring\n    \"\"\"\n    # 1) Init parameters & Adam moments\n    W1, b1, W2, b2 = init_params()\n    mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)\n    mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)\n    mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)\n    mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)\n\n    for t in range(1, iterations + 1):\n        # 2) Forward + backward on TRAINING set\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_train)\n        dW1, db1, dW2, db2 = back_prop(W2, Z1, A1, Z2, A2, X_train, Y_train)\n\n        # 3) Adam moment updates\n        mW1 = beta1*mW1 + (1-beta1)*dW1\n        mb1 = beta1*mb1 + (1-beta1)*db1\n        mW2 = beta1*mW2 + (1-beta1)*dW2\n        mb2 = beta1*mb2 + (1-beta1)*db2\n\n        vW1 = beta2*vW1 + (1-beta2)*(dW1**2)\n        vb1 = beta2*vb1 + (1-beta2)*(db1**2)\n        vW2 = beta2*vW2 + (1-beta2)*(dW2**2)\n        vb2 = beta2*vb2 + (1-beta2)*(db2**2)\n\n        # 4) Bias correction\n        mW1_corr = mW1 / (1 - beta1**t)\n        mb1_corr = mb1 / (1 - beta1**t)\n        mW2_corr = mW2 / (1 - beta1**t)\n        mb2_corr = mb2 / (1 - beta1**t)\n        vW1_corr = vW1 / (1 - beta2**t)\n        vb1_corr = vb1 / (1 - beta2**t)\n        vW2_corr = vW2 / (1 - beta2**t)\n        vb2_corr = vb2 / (1 - beta2**t)\n\n        # 5) Parameter update\n        W1 -=  alpha * mW1_corr / (np.sqrt(vW1_corr) + epsilon)\n        b1 -=  alpha * mb1_corr / (np.sqrt(vb1_corr) + epsilon)\n        W2 -=  alpha * mW2_corr / (np.sqrt(vW2_corr) + epsilon)\n        b2 -=  alpha * mb2_corr / (np.sqrt(vb2_corr) + epsilon)\n\n        # 6) Logging\n        if t % print_every == 0 or t == 1:\n            train_acc = np.mean(np.argmax(A2,axis=0) == Y_train)*100\n\n            # forward on TEST set\n            _, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\n            test_acc = np.mean(np.argmax(A2_test,axis=0) == Y_test)*100\n\n            print(f\"Iteration {t:4d} — accuracy with train data: {train_acc:5.2f}%   accuracy with test data: {test_acc:5.2f}%\")\n\n    return W1, b1, W2, b2\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.230771Z","iopub.status.idle":"2025-05-14T18:15:07.231311Z","shell.execute_reply.started":"2025-05-14T18:15:07.231161Z","shell.execute_reply":"2025-05-14T18:15:07.231179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run Adam\nW1, b1, W2, b2 = gradient_descent_adam(\n    X_train, Yc_train, X_test,  Yc_test,\n\n)\n\n# Evaluate on the test set\n_, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\npreds = np.argmax(A2_test, axis=0)\naccuracy = np.mean(preds == Yc_test)\nprint(f\"Test accuracy with Adam: {accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.234293Z","iopub.status.idle":"2025-05-14T18:15:07.234745Z","shell.execute_reply.started":"2025-05-14T18:15:07.234522Z","shell.execute_reply":"2025-05-14T18:15:07.234543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This model is better than the previous one topping out at about 55% to 60% over 500 iterations. with that said this model is heavily over-fitted as you can see by the accuracy with train data. This needs to be fixed in order to increase the valid accuracy. I will start by adding mini-batches to see if that helps. ","metadata":{}},{"cell_type":"code","source":"# Run the see the current model in action\nshow_random_prediction(\n    W1, b1, W2, b2,\n    X_test, Yc_test,\n    idx_to_char,\n    forward_prop\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.236770Z","iopub.status.idle":"2025-05-14T18:15:07.237395Z","shell.execute_reply.started":"2025-05-14T18:15:07.237146Z","shell.execute_reply":"2025-05-14T18:15:07.237176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adam optimization with mini-batches\ndef gradient_descent_adam_mb(\n    X_train, Y_train, X_test, Y_test,\n    epochs=100, alpha=0.001,\n    beta1=0.9, beta2=0.999, epsilon=1e-8,\n    batch_size=64, print_every=5\n):\n    \"\"\"\n    Adam optimizer with mini-batches and validation logging.\n      - epochs: number of full passes over the data\n      - batch_size: mini-batch size\n      - print_every: log every N epochs\n    \"\"\"\n    # Initialize parameters & moments\n    W1, b1, W2, b2 = init_params()\n    mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)\n    mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)\n    mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)\n    mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)\n    t = 0\n\n    for epoch in range(1, epochs + 1):\n        # Mini-batch updates\n        for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size):\n            t += 1\n            Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, Xb)\n            dW1, db1, dW2, db2 = back_prop(W2, Z1, A1, Z2, A2, Xb, Yb)\n\n            # Adam first moments\n            mW1 = beta1*mW1 + (1-beta1)*dW1\n            mb1 = beta1*mb1 + (1-beta1)*db1\n            mW2 = beta1*mW2 + (1-beta1)*dW2\n            mb2 = beta1*mb2 + (1-beta1)*db2\n\n            # Adam second moments\n            vW1 = beta2*vW1 + (1-beta2)*(dW1**2)\n            vb1 = beta2*vb1 + (1-beta2)*(db1**2)\n            vW2 = beta2*vW2 + (1-beta2)*(dW2**2)\n            vb2 = beta2*vb2 + (1-beta2)*(db2**2)\n\n            # Bias-corrected moments\n            mW1_corr = mW1 / (1 - beta1**t)\n            mb1_corr = mb1 / (1 - beta1**t)\n            mW2_corr = mW2 / (1 - beta1**t)\n            mb2_corr = mb2 / (1 - beta1**t)\n            vW1_corr = vW1 / (1 - beta2**t)\n            vb1_corr = vb1 / (1 - beta2**t)\n            vW2_corr = vW2 / (1 - beta2**t)\n            vb2_corr = vb2 / (1 - beta2**t)\n\n            # Parameter updates\n            W1 -= alpha * mW1_corr / (np.sqrt(vW1_corr) + epsilon)\n            b1 -= alpha * mb1_corr / (np.sqrt(vb1_corr) + epsilon)\n            W2 -= alpha * mW2_corr / (np.sqrt(vW2_corr) + epsilon)\n            b2 -= alpha * mb2_corr / (np.sqrt(vb2_corr) + epsilon)\n\n        # Epoch logging\n        if epoch % print_every == 0 or epoch == 1:\n            _, _, _, A2_tr = forward_prop(W1, b1, W2, b2, X_train)\n            train_acc = np.mean(np.argmax(A2_tr, axis=0) == Y_train) * 100\n            _, _, _, A2_val = forward_prop(W1, b1, W2, b2, X_test)\n            test_acc = np.mean(np.argmax(A2_val, axis=0) == Y_test) * 100\n            print(f\"Epoch {epoch:3d} — accuracy with train data: {train_acc:5.2f}%   accuracy with test data: {test_acc:5.2f}%\")\n\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.238609Z","iopub.status.idle":"2025-05-14T18:15:07.239013Z","shell.execute_reply.started":"2025-05-14T18:15:07.238850Z","shell.execute_reply":"2025-05-14T18:15:07.238868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent_adam_mb(\n    X_train, Yc_train, X_test,  Yc_test,\n)\n\n# Evaluate on the test set\n_, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\npreds = np.argmax(A2_test, axis=0)\naccuracy = np.mean(preds == Yc_test)\nprint(f\"Test accuracy with Adam and mini-batches: {accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.240519Z","iopub.status.idle":"2025-05-14T18:15:07.240854Z","shell.execute_reply.started":"2025-05-14T18:15:07.240685Z","shell.execute_reply":"2025-05-14T18:15:07.240697Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I added mini-batches to try and decrease the over fitting and also increase the accuracy. This did not decrease the overfitting at all but it did increase the accuracy of the model up to a max of about 70% to 75%. To fix the over-fitting I will be doing data augmentation to shift the image which will decrease the over fit and also increase accuracy. ","metadata":{}},{"cell_type":"code","source":"# Run the see the current model in action\nshow_random_prediction(\n    W1, b1, W2, b2,\n    X_test, Yc_test,\n    idx_to_char,\n    forward_prop\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:15:07.242688Z","iopub.status.idle":"2025-05-14T18:15:07.243224Z","shell.execute_reply.started":"2025-05-14T18:15:07.243060Z","shell.execute_reply":"2025-05-14T18:15:07.243077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adam optimization with mini-batches and shift\ndef gradient_descent_adam_mb_aug(\n    X_train, Y_train, X_test, Y_test,\n    epochs=50, alpha=0.001, max_shift=3,\n    beta1=0.9, beta2=0.999, epsilon=1e-8,\n    batch_size=64, print_every=5\n):\n    W1, b1, W2, b2 = init_params()\n    mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)\n    mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)\n    mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)\n    mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)\n    t = 0\n\n    for epoch in range(1, epochs+1):\n        for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size):\n            t += 1\n\n            # 1) augment your batch\n            Xb_aug = augment_batch_shifts(Xb, max_shift=3)\n\n            # 2) forward on the _augmented_ batch\n            Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, Xb_aug)\n\n            # 3) backward on the same augmented batch\n            dW1, db1, dW2, db2 = back_prop(W2, Z1, A1, Z2, A2, Xb_aug, Yb)\n\n            # 4) Adam‐style moment updates\n            mW1 = beta1*mW1 + (1-beta1)*dW1\n            mb1 = beta1*mb1 + (1-beta1)*db1\n            mW2 = beta1*mW2 + (1-beta1)*dW2\n            mb2 = beta1*mb2 + (1-beta1)*db2\n\n            vW1 = beta2*vW1 + (1-beta2)*(dW1**2)\n            vb1 = beta2*vb1 + (1-beta2)*(db1**2)\n            vW2 = beta2*vW2 + (1-beta2)*(dW2**2)\n            vb2 = beta2*vb2 + (1-beta2)*(db2**2)\n\n            # bias‑correction\n            mW1_corr = mW1   / (1 - beta1**t)\n            mb1_corr = mb1   / (1 - beta1**t)\n            mW2_corr = mW2   / (1 - beta1**t)\n            mb2_corr = mb2   / (1 - beta1**t)\n            vW1_corr = vW1   / (1 - beta2**t)\n            vb1_corr = vb1   / (1 - beta2**t)\n            vW2_corr = vW2   / (1 - beta2**t)\n            vb2_corr = vb2   / (1 - beta2**t)\n\n            # 5) update parameters\n            W1 -= alpha * mW1_corr / (np.sqrt(vW1_corr) + epsilon)\n            b1 -= alpha * mb1_corr / (np.sqrt(vb1_corr) + epsilon)\n            W2 -= alpha * mW2_corr / (np.sqrt(vW2_corr) + epsilon)\n            b2 -= alpha * mb2_corr / (np.sqrt(vb2_corr) + epsilon)\n\n        # logging after each epoch\n        if epoch % print_every == 0 or epoch == 1:\n            _, _, _, A2_tr = forward_prop(W1, b1, W2, b2, X_train)\n            train_acc = np.mean(np.argmax(A2_tr,axis=0)==Y_train)*100\n            _, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\n            test_acc   = np.mean(np.argmax(A2_test,axis=0)==Y_test)*100\n            print(f\"Epoch {epoch:3d} — accuracy with train data: {train_acc:5.2f}%   accuracy with test data: {test_acc:5.2f}%\")\n\n    return W1, b1, W2, b2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:27:47.472946Z","iopub.execute_input":"2025-05-14T18:27:47.473303Z","iopub.status.idle":"2025-05-14T18:27:47.486634Z","shell.execute_reply.started":"2025-05-14T18:27:47.473278Z","shell.execute_reply":"2025-05-14T18:27:47.485607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent_adam_mb_aug(\n    X_train, Yc_train, X_test, Yc_test,\n)\n\n# Evaluate on the test set\n_, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\npreds = np.argmax(A2_test, axis=0)\naccuracy = np.mean(preds == Yc_test)\nprint(f\"Test accuracy with Adam, mini-batches and augmentation: {accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T19:04:29.810503Z","iopub.execute_input":"2025-05-14T19:04:29.810832Z","iopub.status.idle":"2025-05-14T19:07:21.485961Z","shell.execute_reply.started":"2025-05-14T19:04:29.810791Z","shell.execute_reply":"2025-05-14T19:07:21.484958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see from the train data accuracy compared to the test data accuracy adding a shift to the pictures helped with the over-fitting problem and also pushed the model to the 85% mark. I want to max out this accuracy so I will now implement an early stop so I can run more epochs and get a higher accuracy at the end as well as adjust the input variables.","metadata":{}},{"cell_type":"code","source":"# Run the see the current model in action\nshow_random_prediction(\n    W1, b1, W2, b2,\n    X_test, Yc_test,\n    idx_to_char,\n    forward_prop\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:27:39.290150Z","iopub.execute_input":"2025-05-14T18:27:39.290426Z","iopub.status.idle":"2025-05-14T18:27:39.347554Z","shell.execute_reply.started":"2025-05-14T18:27:39.290406Z","shell.execute_reply":"2025-05-14T18:27:39.346363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adam optimization with mini-batches and shift with an early stop for best model possible\n# I also tweaked the function input values to get even better results\ndef gradient_descent_adam_mb_aug_earlystop(\n    X_train, Y_train, X_test, Y_test,\n    epochs=1000, alpha=5e-4, max_shift=2,\n    beta1=0.9, beta2=0.995, epsilon=1e-8,\n    batch_size=128, print_every=5,\n    patience=100\n):\n    \"\"\"\n    Mini-batch Adam with data augmentation, early stopping.\n      - patience: # of epochs to wait after last val improvement\n    \"\"\"\n    # Initialize parameters & Adam moments\n    W1, b1, W2, b2 = init_params()\n    mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)\n    mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)\n    mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)\n    mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)\n    t = 0\n\n    best_val = -np.inf\n    best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy())\n    wait = 0\n\n    for epoch in range(1, epochs + 1):\n        for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size):\n            t += 1\n            # augmentation\n            Xb_aug = augment_batch_shifts(Xb, max_shift = 2)\n            # forward/backprop\n            Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, Xb_aug)\n            dW1, db1, dW2, db2 = back_prop(W2, Z1, A1, Z2, A2, Xb_aug, Yb)\n\n            # Adam update steps...\n            mW1 = beta1*mW1 + (1-beta1)*dW1\n            mb1 = beta1*mb1 + (1-beta1)*db1\n            mW2 = beta1*mW2 + (1-beta1)*dW2\n            mb2 = beta1*mb2 + (1-beta1)*db2\n\n            vW1 = beta2*vW1 + (1-beta2)*(dW1**2)\n            vb1 = beta2*vb1 + (1-beta2)*(db1**2)\n            vW2 = beta2*vW2 + (1-beta2)*(dW2**2)\n            vb2 = beta2*vb2 + (1-beta2)*(db2**2)\n\n            mW1_corr = mW1   / (1 - beta1**t)\n            mb1_corr = mb1   / (1 - beta1**t)\n            mW2_corr = mW2   / (1 - beta1**t)\n            mb2_corr = mb2   / (1 - beta1**t)\n            vW1_corr = vW1   / (1 - beta2**t)\n            vb1_corr = vb1   / (1 - beta2**t)\n            vW2_corr = vW2   / (1 - beta2**t)\n            vb2_corr = vb2   / (1 - beta2**t)\n\n            W1 -= alpha * mW1_corr / (np.sqrt(vW1_corr) + epsilon)\n            b1 -= alpha * mb1_corr / (np.sqrt(vb1_corr) + epsilon)\n            W2 -= alpha * mW2_corr / (np.sqrt(vW2_corr) + epsilon)\n            b2 -= alpha * mb2_corr / (np.sqrt(vb2_corr) + epsilon)\n\n        # End of epoch: evaluate validation accuracy\n        _, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\n        test_acc = np.mean(np.argmax(A2_test, axis=0) == Y_test) * 100\n\n        # Check for improvement\n        if  test_acc > best_val:\n            best_val = test_acc\n            best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy())\n            wait = 0\n        else:\n            wait += 1\n\n        # Logging\n        if epoch % print_every == 0 or epoch == 1:\n            _, _, _, A2_tr = forward_prop(W1, b1, W2, b2, X_train)\n            train_acc = np.mean(np.argmax(A2_tr, axis=0) == Y_train) * 100\n            print(f\"Epoch {epoch:3d} — accuracy with train data: {train_acc:5.2f}%  accuracy with test data: {test_acc:5.2f}%\")\n\n        # Early stopping\n        if wait >= patience:\n            print(f\"Early stopping at epoch {epoch}. Best test accuracy: {best_val:.2f}%\")\n            W1, b1, W2, b2 = best_params\n            break\n\n    # Restore best weights\n    W1, b1, W2, b2 = best_params\n    print(f\"Training finished. Best test accuracy: {best_val:.2f}%\")\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T02:03:44.375462Z","iopub.execute_input":"2025-05-15T02:03:44.376239Z","iopub.status.idle":"2025-05-15T02:03:44.391754Z","shell.execute_reply.started":"2025-05-15T02:03:44.376208Z","shell.execute_reply":"2025-05-15T02:03:44.390877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent_adam_mb_aug_earlystop(\n    X_train, Yc_train, X_test, Yc_test,\n)\n\n# Evaluate on the test set\n_, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\npreds = np.argmax(A2_test, axis=0)\naccuracy = np.mean(preds == Yc_test)\nprint(f\"Test accuracy with Adam, mini-batches, data augmentation and early stop: {accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T02:03:55.066596Z","iopub.execute_input":"2025-05-15T02:03:55.066967Z","iopub.status.idle":"2025-05-15T02:13:07.215133Z","shell.execute_reply.started":"2025-05-15T02:03:55.066943Z","shell.execute_reply":"2025-05-15T02:13:07.214198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I added an early stop for the model to stop training once the test accuracy tops out. this way I can get the best model possible while keeping the over fit low. adding the early stop and changing the alpha and beta values as well as doubling the batch amount pushed this model alot and now this model gets over 90% with only a 2 layer network.  ","metadata":{}},{"cell_type":"code","source":"# Run the see the final model in action against the test set\nshow_random_prediction(\n    W1, b1, W2, b2,\n    X_test, Yc_test,\n    idx_to_char,\n    forward_prop\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T02:20:05.172724Z","iopub.execute_input":"2025-05-15T02:20:05.173127Z","iopub.status.idle":"2025-05-15T02:20:05.234844Z","shell.execute_reply.started":"2025-05-15T02:20:05.173097Z","shell.execute_reply":"2025-05-15T02:20:05.234014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test the final model on the validation set\npredict_random_samples(\n    W1, b1, W2, b2,\n    X_val, Yc_val, Yd_val, num_samples=10\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T02:19:51.716663Z","iopub.execute_input":"2025-05-15T02:19:51.717142Z","iopub.status.idle":"2025-05-15T02:19:52.355545Z","shell.execute_reply.started":"2025-05-15T02:19:51.717109Z","shell.execute_reply":"2025-05-15T02:19:52.354308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"9 out of 10 of these statistically will be correct based on the model accuracy. \n","metadata":{}},{"cell_type":"markdown","source":"# Author and Credits\n\n**Kaleb Coleman**  \nData Science Major, Northern Arizona University\n\n**Inspired by**: Video tutorial by Samson Zhang: https://www.youtube.com/watch?v=w8yWXqWQYmU","metadata":{}}]}